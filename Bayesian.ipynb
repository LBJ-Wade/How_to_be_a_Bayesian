{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires **cpnest**; see https://github.com/johnveitch/cpnest. \n",
    "\n",
    "Can be installed via `pip install cpnest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "from corner import corner\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('code'))\n",
    "\n",
    "import cpnest\n",
    "import cpnest.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Analysis\n",
    "\n",
    "### Christopher J. Moore \n",
    "\n",
    "In this session I will give an introduction to some Bayesian methods.\n",
    "The focus will be on the statistical ideas, not the physics. Therefore we will work with a highly simplified toy problem and idealised mock data.\n",
    "\n",
    "**Contents:**\n",
    " - **1. A New Class of Transient**: This section describes the toy problem, the mock data and the models we will be using.\n",
    " - **2. Analysing Individual Events**: This section applyies Bayesian analysis to the analysis of an individual transient light curve: detection, parameter estimation, and model selection are all discussed. \n",
    " - **3. Hierarchical Bayesian Analysis**: Given several light curves we can start asking questions about the underlying population. This section analyses the population of several events in a mock catalog\n",
    " - **4. Conclusions**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: A New Class of Transient\n",
    "\n",
    "Supppose that we observe some new type of transient event. We want to analyse a small catalog containing the light curves from the first such events.\n",
    "\n",
    "## Section 1.1: Data\n",
    "\n",
    "The light curve data for the small catalog of transietns is stored in *data/catalog.dat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/catalog.dat\") as f:\n",
    "    for i in range(7): # print first few lines\n",
    "        line = f.readline()[0:-1]\n",
    "        \n",
    "        # Read the error from the header \n",
    "        if 'sigma' in line: sigma = float(line.split()[-1])\n",
    "        \n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column 0:** time stamps. (For simplicity, times are shifted such that peak occurs at $t=0$ and resampled at 1Hz.)\n",
    "\n",
    "**Columns 1 to Nevents:** light curves for first few events. (Each flux measurement is assumed to have an identical, independent Gaussian error which is given in the file.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_data = np.loadtxt(\"data/catalog.dat\")\n",
    "\n",
    "times = catalog_data[:,0]\n",
    "Nevents = catalog_data.shape[1] - 1\n",
    "\n",
    "for N in range(Nevents):\n",
    "    plt.plot(times, catalog_data[:,N+1], label='event '+str(N))  \n",
    "    \n",
    "plt.xlabel(\"Time [s]\")\n",
    "plt.ylabel(\"Flux [arb units]\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Model(s)\n",
    "\n",
    "The simplest possible situation is that there is no signal present in the data (null hypothesis).\n",
    "\n",
    "**Model 0** is the null hypothesis,\n",
    "$$ F_0(t) = 0 $$\n",
    "\n",
    "Now suppose that there are two competing models for the light curve signals. \n",
    "\n",
    "**Model A** is a Gaussian, \n",
    "$$ F_A(t) = A\\exp\\left( \\frac{-t^2}{\\tau^2} \\right) \\,.$$\n",
    "\n",
    "**Model B** has a similar shape but with broader tails,\n",
    "$$ F_B(t) = \\frac{A \\tau^2}{\\tau^2+t^2 } \\,. $$\n",
    "\n",
    "Both signal hypotheses have two parameters: an amplitude $A$, and a duration $\\tau$; we collectively denote the model parameters\n",
    "$\\theta^{\\mu} = (A, \\tau)$.\n",
    "\n",
    "These models are coded up in *Models.py*; let's import and inspect these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import ModelA_lightcurve as ModelA\n",
    "from Models import ModelB_lightcurve as ModelB\n",
    "\n",
    "print('def ModelA(times, params):\\n    \"\"\"', ModelA.__doc__[:-1], '\"\"\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.0: Analysing Individual Events \n",
    "\n",
    "## Section 2.1: Bayes' Theorem\n",
    "\n",
    "Bayes' theorem is the basis for everything we do in the this notebook;\n",
    "$$ P(\\theta^{\\mu} | \\mathrm{data}) = P(\\mathrm{data} | \\theta^{\\mu}) \\frac{P(\\theta^{\\mu}_{i})}{P(\\mathrm{data})} \\,. $$\n",
    "This equation is to be understood as appling to a particular event; i.e. $\\theta=\\theta_i$ for $i\\in\\{1,\\ldots, N_\\rm{events}\\}$ and $\\mathrm{data}=\\mathrm{data}_i$. \n",
    "\n",
    "Each term has a special name:\n",
    "\n",
    "#### The Posterior: $P(\\theta| \\rm{data} )$\n",
    "\n",
    "This is what we want to find (sometimes known as the target distribution). It is the probability of the model parameters given the data.\n",
    "\n",
    "#### The Likelihood: $P(\\rm{data} | \\theta) \\equiv \\mathcal{L}(\\theta)$\n",
    "\n",
    "This is a input to the Bayesian analysis.\n",
    "\n",
    "The probability of the data given a particular set of model parameters.\n",
    "\n",
    "For our toy problem, because we have assumed simple noise (i.e. identical, independent Gaussian etc) the likelihood takes a simple form;\n",
    "$$ \\log\\mathcal{L}(\\theta) = \\frac{-1}{2}\\sum_{m=1}^{N_\\mathrm{times}}\\frac{\\big[\\mathrm{data}_m-\\mathrm{model}(\\theta)_m)\\big]^2}{\\sigma^{2}} -\\frac{N_\\mathrm{times}}{2}\\log(2\\pi\\sigma^2) \\,. $$\n",
    "\n",
    "#### The Prior: $P(\\theta)\\equiv \\Pi(\\theta)$\n",
    "\n",
    "This is a input to the Bayesian analysis.\n",
    "\n",
    "The prior probability on the source parameters. There are no concrete rules for how to choose priors. \n",
    "However, a reasonable choice here would be a seperable prior on the two parameters; i.e. \n",
    "$$\\Pi(\\theta^{\\mu})=\\Pi(A)\\Pi(\\tau)\\,.$$\n",
    "For the amplitude the prior is uniform in log (Jeffreys \n",
    "prior) between lower/upper cutoffs $A_\\mathrm{lower}=0.03$ and $A_\\mathrm{upper}=30$;\n",
    "$$\\Pi(A)\\propto\\frac{1}{\\log\\left(A_\\mathrm{upper}/A_\\mathrm{lower}\\right)}\\begin{cases}1/A & \\rm{if}\\;A >A_\\mathrm{lower}\\;\\rm{and}\\;A <A_\\mathrm{upper}\\\\0 &\\rm{else}\\end{cases}\\,,$$\n",
    "and for the duration the prior is uniform between lower/upper cutoffs $\\tau_\\mathrm{lower}=3$ and $\\tau_\\mathrm{upper}=30$;\n",
    "$$\\Pi(\\tau)\\propto \\frac{1}{\\tau_\\mathrm{upper}-\\tau_\\mathrm{lower}}\\begin{cases}1& \\rm{if}\\;\\tau>1\\;\\rm{and}\\;\\tau<100\\\\0 &\\rm{else}\\end{cases}\\,.$$\n",
    "\n",
    "#### The Evidence: $P(\\rm{data})\\equiv Z$\n",
    "\n",
    "This is the normalisation factor for the posterior,\n",
    "$$ Z = \\int\\mathrm{d}\\theta^{\\mu} \\; \\mathcal{L}(\\theta^{\\mu})\\Pi(\\theta^{\\mu}) \\,. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "In the new notation, Bayes' theorem becomes\n",
    "$$ P(\\theta^{\\mu} | \\mathrm{data}) = \\frac{\\mathcal{L}(\\theta^{\\mu}) \\Pi(\\theta^{\\mu})}{Z} \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: CPNest Implementation\n",
    "\n",
    "The **likelihood** and **prior** functions are the inputs to any Bayesian analysis. Here these functions are implemented inside the *model* class of the *CPNest* python package.\n",
    "\n",
    "CPNest implements the nested sampling algorithm [J. Skilling (2006) doi:10.1214/06-BA127]. Nested sampling calculates $Z$ by Monte-Carlo integration of $\\int\\mathrm{d}\\theta \\; \\mathcal{L}(\\theta)\\Pi(\\theta)$. As by product, this algorithm produces a list parameter values $\\{\\theta_{1}, \\theta_2, \\ldots, \\theta_{N_\\rm{samples}}\\}$ which are distributed according to the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNestModel(cpnest.model.Model):\n",
    "    \n",
    "    def __init__(self, catalog_data, event_num=0, model='A', sigma=0.1):\n",
    "        \"\"\"\n",
    "        INPUTS\n",
    "        ------\n",
    "        catalog_data: numpy array\n",
    "            the time series data for all light curves\n",
    "        event_num: int\n",
    "            which event to analyse? e.g. 1, 2,... [default to 1]\n",
    "        model: string\n",
    "            which model to use? e.g. 'A' or 'B' or 'Null' [defult to 'A']\n",
    "        sigma: float\n",
    "            the 1-sigma uncertainty on the measurements\n",
    "        \"\"\"\n",
    "        \n",
    "        self.catalog = catalog_data\n",
    "        \n",
    "        self.event = event_num\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.sigma = sigma\n",
    "        \n",
    "        if model is 'Null': # null hypothesis\n",
    "            self.names   = []\n",
    "            self.bounds  = []\n",
    "            \n",
    "        else: # signal hypotheses\n",
    "            self.names   = ['amplitude', 'duration']\n",
    "            self.bounds  = [[0.03, 30], [3, 30]]\n",
    "    \n",
    "    def log_prior(self, params):\n",
    "        \"\"\"\n",
    "        The log-prior distribution\n",
    "\n",
    "        INPUTS\n",
    "        ------\n",
    "        params: dict of model parameters\n",
    "            keys 'amplitude' and 'duration'\n",
    "    \n",
    "        RETURNS\n",
    "        -------\n",
    "            LogPrior: float\n",
    "        \"\"\"\n",
    "        if self.model is 'Null':\n",
    "            return 0\n",
    "        elif not self.in_bounds(params): \n",
    "            return -np.inf\n",
    "        else:\n",
    "            LogPrior = -np.log(np.log(self.bounds[0][1]/self.bounds[0][0]))\n",
    "            LogPrior -= np.log(self.bounds[1][1]-self.bounds[1][0])\n",
    "            return LogPrior \n",
    "        \n",
    "    def log_likelihood(self, params):\n",
    "        \"\"\"\n",
    "        The log-likelihood distribution\n",
    "    \n",
    "        LogLike = sum_i ( -0.5*(data_i-model_i)**2/sigma**2 ) - norm_const\n",
    "    \n",
    "        INPUTS\n",
    "        ------\n",
    "        params: dict of model parameters\n",
    "            keys 'amplitude' and 'duration'\n",
    "    \n",
    "        RETURNS\n",
    "        -------\n",
    "            LogLike: float\n",
    "        \"\"\"\n",
    "        times = self.catalog[:,0]\n",
    "        data_light_curve = self.catalog[:,self.event+1]\n",
    "    \n",
    "        if self.model=='Null':\n",
    "            model_lightcurve = np.zeros(len(times))\n",
    "        elif self.model=='A':\n",
    "            model_lightcurve = ModelA(times, params)\n",
    "        elif self.model=='B':\n",
    "            model_lightcurve = ModelB(times, params)\n",
    "\n",
    "        LogLike = -0.5 * np.sum( (data_light_curve-model_lightcurve)**2 / self.sigma**2 )\n",
    "        LogLike -= 0.5*len(times)*np.log(2*np.pi*self.sigma**2)\n",
    "    \n",
    "        return LogLike\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Analysing First Event With Model A\n",
    "\n",
    "Now we have set up the CPNest class it is a simple matter to create an instance of this class for a particular event (say, event number 0) and run the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventID = 0\n",
    "modelA = CPNestModel(catalog_data, \n",
    "                     event_num=eventID, \n",
    "                     model='A',\n",
    "                     sigma=sigma)\n",
    "\n",
    "nest_modelA = cpnest.CPNest(modelA, \n",
    "                            nlive=1024, \n",
    "                            output='results/event'+str(eventID)+'_A', \n",
    "                            nthreads=4, \n",
    "                            verbose=1)\n",
    "\n",
    "nest_modelA.run()\n",
    "posterior_samplesA = nest_modelA.get_posterior_samples() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.1: Parameter Estimation\n",
    "\n",
    "As mentioned above, the nested sampling algorithm produces (as a by product) as list of *posterior samples*. This is a list of parameter values, each of witch is an independant random draw from the posterior (AKA target distribution). We can visualise the posterior by simply plotting a histogram of the posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array(list(zip(*[posterior_samplesA[name] for name in modelA.names])))\n",
    "corner(samples, labels=modelA.names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.2: Detection\n",
    "\n",
    "We can use the Bayesian evidence to decide whether or not there is a signal present.\n",
    "\n",
    "First, compute the evidence for hypothesis A, defined as\n",
    "$$Z_{A} = \\int\\mathrm{d}\\theta^{\\mu}\\;\\mathcal{L}_{A}(\\theta^{\\mu})\\Pi(\\theta^{\\mu}) \\,. $$ \n",
    "This has already been done above by CPNest.\n",
    "\n",
    "Second, compute the evidence for the null hypothesis. The null hypothesis has no parameters (i.e. $\\theta=\\{\\}$) so the evidence is simply given by the likelihood,\n",
    "\n",
    "$$ Z_0 = \\mathcal{L}_{0}() \\,. $$\n",
    "\n",
    "(This is an example of a Savage-Dickey density ratio.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = CPNestModel(catalog_data, \n",
    "                     event_num=eventID, \n",
    "                     model='Null')\n",
    "\n",
    "folder = 'results/event'+str(eventID)+'_0/'\n",
    "Z0 = np.array([ model0.log_likelihood({}), 0])\n",
    "\n",
    "print('Computed log_evidences: {}'.format(Z0[0]))\n",
    "os.makedirs(folder, exist_ok=True); np.savetxt(folder+'evidence.txt', Z0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the *odds ratio* between the two hypothesis is given by the ratio\n",
    "\n",
    "$$ \\mathcal{O}_{A0} = \\frac{Z_A}{Z_0} \\,. $$\n",
    "\n",
    "If this ratio is larger than unity then there is evidence in favour of a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import GetEvidence\n",
    "\n",
    "LogOddsRatio = GetEvidence(eventID, model='A') - GetEvidence(eventID, model='0')\n",
    "\n",
    "print(\"log(O_A0) = \", LogOddsRatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **huge** number; there is definitely a signal present. The null hypothesis is ruled out at extremely high significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2.3: Model Selection\n",
    "\n",
    "We can also use the Bayesian evidence to decide between two competing signal models.\n",
    "\n",
    "We compute the evidences for each model, $Z_A$ and $Z_B$, and form the *odds ratio*\n",
    "\n",
    "$$ \\mathcal{O}_{AB} = \\frac{Z_A}{Z_B} \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CPNest model\n",
    "modelB = CPNestModel(catalog_data, \n",
    "                     event_num=eventID, \n",
    "                     model='B')\n",
    "\n",
    "# Set up the nested sampling algorithm\n",
    "nest_modelB = cpnest.CPNest(modelB, \n",
    "                            nlive=1024, \n",
    "                            output='results/event'+str(eventID)+'_B', \n",
    "                            nthreads=4, \n",
    "                            verbose=0)\n",
    "\n",
    "# Run the algorithm\n",
    "nest_modelB.run()\n",
    "posterior_samplesB = nest_modelB.get_posterior_samples() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogOddsRatio = GetEvidence(eventID, model='A') - GetEvidence(eventID, model='B')\n",
    "print(\"log(O_AB) = \", LogOddsRatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is very strong evidence in favour of model B.\n",
    "\n",
    "Now that we have run CPNest for model B, we can look at the posterior distribution for the parameters $\\theta^\\mu$ under this model. The posterior on the parameters for model B is not the same as that for model A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(list(zip(*[posterior_samplesB[name] for name in modelB.names]))), \n",
    "       labels=modelB.names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Repeat Analysis for all Events in Catalog \n",
    "\n",
    "I ran this earlier for all events. The results are in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eventID = 1\n",
    "\n",
    "# Define the CPNest model A\n",
    "modelA = CPNestModel(catalog_data,\n",
    "                     event_num=eventID,\n",
    "                     model='A')\n",
    "\n",
    "nest_modelA = cpnest.CPNest(modelA,\n",
    "                            nlive=1024,\n",
    "                            output='results/event'+str(eventID)+'_A',\n",
    "                            nthreads=4,\n",
    "                            verbose=0)\n",
    "\n",
    "nest_modelA.run()\n",
    "post = nest_modelA.get_posterior_samples()\n",
    "\n",
    "# Define the CPNest model B\n",
    "modelB = CPNestModel(catalog_data,\n",
    "                     event_num=eventID,\n",
    "                     model='B')\n",
    "\n",
    "nest_modelB = cpnest.CPNest(modelB,\n",
    "                            nlive=1024,\n",
    "                            output='results/event'+str(eventID)+'_B',\n",
    "                            nthreads=4,\n",
    "                            verbose=0)\n",
    "\n",
    "nest_modelB.run()\n",
    "post = nest_modelB.get_posterior_samples()\n",
    "\n",
    "# The null hypothesis\n",
    "model0 = CPNestModel(catalog_data,\n",
    "                     event_num=eventID,\n",
    "                     model='Null')\n",
    "\n",
    "folder = 'results/event'+str(eventID)+'_0/'\n",
    "Z0 = np.array([ model0.log_likelihood({}), 0])\n",
    "\n",
    "print('Computed log_evidences: ({},)'.format(Z0))\n",
    "os.makedirs(folder, exist_ok=True); np.savetxt(folder+'evidence.txt', Z0)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Uncomment and run repeatedly with eventID 1,2,...,9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4: Summary Plots for our mock catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the posterior samples from any of the events in the catalog and plot a corner plot if you wish to see the posterior distributions.\n",
    "\n",
    "Here I will load the evidences for each hypothesis and for each event ($3\\, \\mathrm{hypotheses }\\times 10\\,\\mathrm{events}=30$ evidences in total) and plot these to see if we can reach any general conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zs = np.array([ [GetEvidence(eventID, model='0'), \n",
    "                 GetEvidence(eventID, model='A'), \n",
    "                 GetEvidence(eventID, model='B')] for eventID in range(10)])\n",
    "\n",
    "logO_A0 = Zs[:,1] - Zs[:,0]\n",
    "logO_BA = Zs[:,2] - Zs[:,1]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "axes[0].errorbar(np.arange(Nevents), np.log(np.maximum(1,logO_A0)), \n",
    "                 xerr=0., yerr=0.0, uplims=logO_A0<1, marker='+', linestyle='',\n",
    "                 color='red', label='log(O_A0)')\n",
    "axes[0].errorbar(np.arange(Nevents), np.log(np.maximum(1,logO_BA)), \n",
    "                 xerr=0., yerr=0.0, uplims=logO_BA<1, marker='+', linestyle='',\n",
    "                 color='blue', label='log(O_BA)')\n",
    "axes[0].set_xlabel(\"Event ID\", fontsize=BIGGER_SIZE)\n",
    "axes[0].set_ylabel(\"Log ( Log Odds Ratio )\", fontsize=BIGGER_SIZE)\n",
    "axes[0].legend(loc='upper right', fontsize=MEDIUM_SIZE)\n",
    "\n",
    "for N in range(Nevents):\n",
    "    axes[1].plot(times, catalog_data[:,N+1], label='event '+str(N))  \n",
    "axes[1].set_xlabel(\"Time [s]\", fontsize=BIGGER_SIZE)\n",
    "axes[1].set_ylabel(\"Flux\", fontsize=BIGGER_SIZE)\n",
    "axes[1].legend(loc='upper right', fontsize=MEDIUM_SIZE)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that hypothesis B is generally prefered. These light curves are not Gaussian (with the possible exception of event 8 which is extremely faint in any case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Hierarchical Bayesian Models\n",
    "\n",
    "Inevitably, as we observe more events and the size of our catalog grows, we will stop just asking questions about individual events and start to ask questions about the population as a whole. The formalism of Bayesian analysis can be repurposed to address these questions as well.\n",
    "\n",
    "In the above analysis a flat prior on the duration was used. But we might wonder whether this really reflects the population in our catalog. Certainly, looking at the light curves it seems that the durations are all clustered around a similar length. What is this average duration and how tight is the clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: General Approach\n",
    "\n",
    "We could rerun the above analyses with a new prior on the parameters\n",
    "$$ \\Pi(\\theta^\\mu) \\rightarrow \\Pi(\\theta^\\mu|\\lambda^{\\alpha}) \\,, $$\n",
    "where the new *hyperparameters* $\\lambda^{\\alpha}$ describe the population.\n",
    "\n",
    "Bayes' theorem for an individual event now reads\n",
    "$$ P\\big(\\theta^\\mu_i|\\mathrm{data}_i, \\lambda^\\alpha \\big) = \\frac{P\\big(\\mathrm{data}_i|\\theta^\\mu\\big)\\Pi\\big(\\theta^\\mu | \\lambda^\\alpha\\big)}{Z_i(\\lambda^\\alpha)} \\,, $$\n",
    "for a fixed $i$ and where the $\\lambda^\\alpha$ are viewed as being fixed (constant). The normalising evidence now depends on the hyperparameters $\\lambda^\\alpha$.\n",
    "\n",
    "Bayes' theorem one level up! Bayes' theorem applied to the population reads\n",
    "$$ P\\big(\\lambda^\\alpha|\\{\\mathrm{data}\\} \\big) = \\frac{P\\big(\\{\\mathrm{data}\\}|\\lambda^\\alpha\\big)\\Pi\\big(\\lambda^\\alpha\\big)}{Z} \\,, $$\n",
    "where the $\\lambda^\\alpha$ are now our new *hyperparameters* and $\\Pi(\\lambda^{\\alpha})$ are the *hyperpriors* on these parameters.\n",
    "At this new, higher level the *hyperlikehood* is given by\n",
    "\n",
    "$$ P(\\lambda^{\\alpha} | \\{\\mathrm{data}\\}) = \\prod_{i=1}^{N_\\mathrm{events}}Z_i(\\lambda^\\alpha) \\,. $$\n",
    "which, by the definition of $Z_{i}(\\lambda^\\alpha)$ becomes\n",
    "$$ P(\\lambda^{\\alpha} | \\{\\mathrm{data}\\}) = \\prod_{i=1}^{N_\\mathrm{events}}\\int\\mathrm{d}\\theta^\\mu\\; P(\\mathrm{data}_i|\\theta^{\\mu})\\Pi(\\theta^\\mu|\\lambda^\\alpha)\\,. $$\n",
    "The posterior samples which we have already obtained provide a computationally efficient method for evaluating this *hyperlikelihood*;\n",
    "$$ P(\\lambda^{\\alpha} | \\{\\mathrm{data}\\}) = \\sum_{i=1}^{N_\\mathrm{events}}\\log\\left( \\sum_{a=1}^{N_\\mathrm{samples}} \\frac{\\Pi(\\theta^{\\mu}_{a}| \\lambda^\\alpha)}{\\Pi(\\theta_a)} \\right) \\,. $$\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "For completeness, we also give here the expressions for the *hyperevidence*, although it will not be needed;\n",
    "\n",
    "$$ Z = \\int\\mathrm{d}\\lambda\\;P(\\lambda^{\\alpha} | \\{\\mathrm{data}\\})\\Pi(\\lambda^{\\alpha}) \\,.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Modelling the Population - Specific example\n",
    "\n",
    "We will choose as our model for the population (AKA *hyperprior*) to be\n",
    "$$ \\Pi(\\theta^{\\mu}|\\lambda^{\\nu}) = \\Pi(A)\\frac{\\exp\\left(\\frac{-(\\tau-\\tau_0)^2}{2\\Delta^2}\\right)}{\\sqrt{2\\pi\\Delta^2}}$$\n",
    "where $\\Pi(A)$ is the Jeffreys prior defined above and $\\lambda^{\\nu}=\\{\\tau_0, \\Delta\\}$.\n",
    "\n",
    "In other words, we are modelling the population of transients as having durations $\\tau$ which are log-normally distributed and we are trying to use the catalog of the first few events to measure the mean, $\\tau_0$, and spread, $\\Delta$, of the durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_samples = [np.loadtxt('results/event'+str(eventID)+'_B/posterior.dat') \n",
    "                   for eventID in range(Nevents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPNestHyperModel(cpnest.model.Model):\n",
    "    \n",
    "    def __init__(self, catalog_samples):\n",
    "        \"\"\"\n",
    "        INPUTS\n",
    "        ------\n",
    "        catalog_sample: list\n",
    "            list length Nevents\n",
    "            each entry in list is a np.array of the posterior samples\n",
    "        \"\"\"\n",
    "        self.catalog_samples = catalog_samples\n",
    "        \n",
    "        self.names   = ['Tau0', 'Delta']\n",
    "        self.bounds  = [[5, 15], [0.5, 2]] # Here I choose flat hyperpriors on \n",
    "                                           # the hyperparameters Tau0 and Delta and\n",
    "                                           # relatively narrow ranges. \n",
    "            \n",
    "    def log_prior(self, params):\n",
    "        \"\"\"\n",
    "        The log-hyperprior distribution\n",
    "\n",
    "        INPUTS\n",
    "        ------\n",
    "        params: dict of model parameters\n",
    "            keys 'Tau0' and 'Delta'\n",
    "        \n",
    "        RETURNS\n",
    "        -------\n",
    "            LogPrior: float\n",
    "        \"\"\"\n",
    "        if not self.in_bounds(params): \n",
    "            return -np.inf\n",
    "        else:\n",
    "            LogPrior = -np.log(self.bounds[0][1]-self.bounds[0][0])\n",
    "            LogPrior -= np.log(self.bounds[1][1]-self.bounds[1][0])\n",
    "            return LogPrior \n",
    "        \n",
    "    def log_likelihood(self, params):\n",
    "        \"\"\"\n",
    "        The log-hyperlikelihood distribution\n",
    "    \n",
    "        INPUTS\n",
    "        ------\n",
    "        params: dict of model parameters\n",
    "            keys 'Tau0' and 'Delta'\n",
    "           \n",
    "        RETURNS\n",
    "        -------\n",
    "            LogLike: float\n",
    "        \"\"\"\n",
    "        LogLike = 0.\n",
    "        for samples in self.catalog_samples:\n",
    "            LogLike += np.log(np.sum(\n",
    "                norm.pdf(samples[:,1], loc=params['Tau0'], scale=params['Delta'])\n",
    "            ) / (1./99.))\n",
    "        \n",
    "        return LogLike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the posterior samples for all of the individual events in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_samples = [np.loadtxt('results/event'+str(eventID)+'_B/posterior.dat') \n",
    "                   for eventID in range(Nevents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the population analysis. We use the machinery of CPNest exactly as before, it is only the interpretation of the various quantities that has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = CPNestHyperModel(catalog_samples)\n",
    "\n",
    "nest_hypermodel = cpnest.CPNest(hypermodel, \n",
    "                            nlive=1024, \n",
    "                            output='results/population_analysis', \n",
    "                            nthreads=4, \n",
    "                            verbose=1)\n",
    "\n",
    "nest_hypermodel.run()\n",
    "nest_hypermodel_samples = nest_hypermodel.get_posterior_samples() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(list(zip(*[nest_hypermodel_samples[name] for name in hypermodel.names]))), \n",
    "       labels=hypermodel.names\n",
    "       #, truths=[10, 1]\n",
    "      )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above corner plot, the events have mean duration of around $(9.4\\pm 0.4)\\,$s with a spread of $(1.2\\pm0.3)\\,$s. \n",
    "\n",
    "Because these are fake events which I generated myself, we can check that these results are consistent with a population I simulated (uncomment the \"truths\" line in the above cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4.0 Conclusions\n",
    "\n",
    "See the lecture slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
